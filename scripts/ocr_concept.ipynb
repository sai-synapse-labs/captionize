{"cells":[{"cell_type":"markdown","metadata":{"id":"KCYz0cGiJnBg"},"source":["# OCR Validator\n","This notebook contains a simple conceptual framework an optical character recognition (OCR) system. It is a part of the OCR subnet tutorial which can be found at XXXXX.\n","\n","A notebook such as this one is a suitable starting point for building a subnet. This is because it contains a well-defined incentive mechanism, which is the essence of a subnet.\n","\n","# Contents\n","1. Define validation flow (noisy images with text)\n","2. Define incentive mechanism (a reward function)\n","3. Define miner (pytesseract)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25570,"status":"ok","timestamp":1702505778039,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"NBY_hHYD2eyH","outputId":"8eddd8c0-8318-4adf-be37-7e982a682719"},"outputs":[],"source":["!pip install h5py\n","!pip install typing-extensions\n","!pip install wheel\n","!pip install bittensor PyMuPDF editdistance reportlab Pillow faker scipy plotly nbformat"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1702505778039,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"igRuJy2FwzXS"},"outputs":[],"source":["import os\n","import io\n","import math\n","import time\n","import fitz\n","import torch\n","import random\n","import datetime\n","import editdistance\n","\n","import bittensor as bt\n","import plotly.express as px\n","\n","from faker import Faker\n","from typing import List\n","from PIL import Image, ImageFilter, ImageDraw\n","from scipy.optimize import linear_sum_assignment\n","\n","from reportlab.lib.pagesizes import letter\n","from reportlab.pdfgen import canvas\n","from reportlab.pdfbase import pdfmetrics\n","\n","seed = 0\n","fake = Faker()\n","# Seed the Faker instance\n","fake.seed_instance(seed)\n","\n","# set random seed\n","random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"M8Cf2XVUJnBh"},"source":["# 1. Validation flow\n","\n","Validation in this subnet consists of creating a pdf document (synthetic data generation) with ground truth data (pdf contents), and scoring miner responses based on how accurately they extract the contents. This can be thought of as a straightforward supervised learning problem.\n","\n","### Synthetic data generation\n","We choose to create synthetic invoice documents as the basis for validation. Faker is a very useful library that can be used to generate a plethora of different data types, and it is ideal for our quick prototype."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1702505778039,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"HwuRtYb_JnBi","outputId":"812a36ea-2207-40fa-82d1-3f0f96119792"},"outputs":[],"source":["# example usage of faker\n","for _ in range(5):\n","    print(f'Name: {fake.name():<20}, Phone: {fake.phone_number():<24}, City: {fake.city():<12}')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1702505778039,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"SLc_G40-JnBi"},"outputs":[],"source":["def create_invoice(invoice_data, filename):\n","    \"\"\"\n","    Generates an invoice from raw data and saves as pdf\n","\n","    Args:\n","    - invoice_data (dict): contents of invoice\n","    - filename (str): path to save pdf file\n","\n","    Returns:\n","    - List[dict]: contents of invoice with text, position and font information for each section\n","    \"\"\"\n","\n","    c = canvas.Canvas(filename, pagesize=letter)\n","    w, h = c._pagesize\n","    c.setLineWidth(.3)\n","\n","    font_name = random.choice(['Helvetica','Times-Roman'])\n","    font_size = random.choice([10, 11, 12])\n","    c.setFont(font_name, font_size)\n","\n","    data = []\n","    def write_text(x, y, text):\n","        c.drawString(x, y, text)\n","        # scale x and y by the page size and estimate bounding box based on font size\n","        # position = [x0, y0, x1, y1]\n","        text_width = pdfmetrics.stringWidth(text, font_name, font_size)\n","        position = [\n","            x/w,\n","            1 - (y - 0.2*font_size)/h,\n","            (x + text_width)/w,\n","            1 - (y + 0.8*font_size)/h\n","        ]\n","\n","        data.append({'position': position, 'text': text, 'font': {'family': font_name, 'size': font_size}})\n","\n","    # Draw the invoice header\n","    write_text(30, 750, invoice_data['company_name'])\n","\n","    write_text(400, 750, \"Invoice Date: \" + invoice_data['invoice_date'])\n","    write_text(400, 735, \"Invoice #: \" + invoice_data['invoice_number'])\n","\n","    write_text(30, 735, invoice_data['company_address'])\n","    write_text(30, 720, invoice_data['company_city_zip'])\n","\n","    # Draw the bill to section\n","    write_text(30, 690, \"Bill To:\")\n","    write_text(120, 690, invoice_data['customer_name'])\n","\n","    # Table headers\n","    write_text(30, 650, \"Description\")\n","    write_text(300, 650, \"Qty\")\n","    write_text(460, 650, \"Cost\")\n","    c.line(30, 645, 560, 645)\n","\n","    # List items\n","    line_height = 625\n","    total = 0\n","    for item in invoice_data['items']:\n","        write_text(30, line_height, item['desc'])\n","        write_text(300, line_height, str(item['qty']))\n","        write_text(460, line_height, \"${:.2f}\".format(item['cost']))\n","        total += item['qty'] * item['cost']\n","        line_height -= 15\n","\n","    # Draw the total cost\n","    write_text(400, line_height - 15, f\"Total: ${total:,.2f}\" )\n","\n","    # Terms and Conditions\n","    write_text(30, line_height - 45, \"Terms:\")\n","    write_text(120, line_height - 45, invoice_data['terms'])\n","\n","    c.save()\n","    return data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1702505778039,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"QadtLYrcJnBi","outputId":"b0a8469c-78dd-4469-dd03-0e4892dce024"},"outputs":[],"source":["# Create a list of optional items for the invoice\n","items_list = [\n","    {\"desc\": \"Web hosting\", \"cost\": 100.00},\n","    {\"desc\": \"Domain registration\", \"cost\": 10.00},\n","    {\"desc\": \"SSL certificate\", \"cost\": 5.50},\n","    {\"desc\": \"Web design\", \"cost\": 500.00},\n","    {\"desc\": \"Web development\", \"cost\": 500.00},\n","    {\"desc\": \"SEO\", \"cost\": 100.00},\n","    {\"desc\": \"Content creation\", \"cost\": 300.00},\n","    {\"desc\": \"Social media marketing\", \"cost\": 400.00},\n","    {\"desc\": \"Email marketing\", \"cost\": 150.00},\n","    {\"desc\": \"PPC advertising\", \"cost\": 200.00},\n","    {\"desc\": \"Analytics\", \"cost\": 400.00},\n","    {\"desc\": \"Consulting\", \"cost\": 700.00},\n","    {\"desc\": \"Training\", \"cost\": 1200.00},\n","    {\"desc\": \"Maintenance\", \"cost\": 650.00},\n","    {\"desc\": \"Support\", \"cost\": 80.00},\n","    {\"desc\": \"Graphic design\", \"cost\": 310.00},\n","    {\"desc\": \"Logo design\", \"cost\": 140.00},\n","    {\"desc\": \"Branding\", \"cost\": 750.00},\n","]\n","\n","def random_items(n):\n","    \"\"\"Selects n items at random (with exclusion) and adds a random quantity\"\"\"\n","    items = sorted(random.sample(items_list, k=n), key=lambda x: x['desc'])\n","    return [{**item, 'qty':random.randint(1,5)} for item in items]\n","\n","random_items(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1702505778039,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"l4I7oDIXJnBi","outputId":"43ace718-b4e9-4572-dc9d-f97949aaa7cc"},"outputs":[],"source":["\n","# Sample data for the invoice, populated with faker\n","invoice_info = {\n","    \"company_name\": fake.company(),\n","    \"company_address\": fake.address(),\n","    \"company_city_zip\": f'{fake.city()}, {fake.zipcode()}',\n","    \"company_phone\": fake.phone_number(),\n","    \"customer_name\": fake.name(),\n","    \"invoice_date\": datetime.date.fromtimestamp(1700176424-random.random()*5e8).strftime(\"%B %d, %Y\"),\n","    \"invoice_number\": f\"INV{random.randint(1,10000):06}\",\n","    \"items\": random_items(random.randint(5,20)),\n","    \"terms\": f\"Payment due within {random.choice([7, 14, 30, 60, 90])} days\"\n","}\n","\n","# Fille the invoice template with our synthetic data and save as a pdf\n","pdf_filename = \"sample_invoice.pdf\"\n","data = create_invoice(invoice_info, pdf_filename)\n","\n","# returned data is the 'ground truth' labels which are used to score miner responses\n","data[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":809},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702505778039,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"W7q-ehgQBSi6","outputId":"6f66d296-e6da-4738-8b89-65510bdac0e1"},"outputs":[],"source":["def load_image(pdf_path, page=0, zoom_x=1.0, zoom_y=1.0):\n","    \"\"\"Loads pdf image and converts to PIL image\"\"\"\n","    # Read the pdf into memory\n","    pdf = fitz.open(pdf_path)\n","    page = pdf[page]\n","\n","   # Set zoom factors for x and y axis (1.0 means 100%)\n","    mat = fitz.Matrix(zoom_x, zoom_y)\n","    pix = page.get_pixmap(matrix=mat)\n","    img_data = io.BytesIO(pix.tobytes('png'))\n","\n","    # convert to PIL image\n","    return Image.open(img_data)\n","\n","image = load_image(pdf_filename)\n","image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2276,"status":"ok","timestamp":1702505780309,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"o5UigKWIJnBi","outputId":"76cfe478-22f7-45c3-a0fe-65420c309509"},"outputs":[],"source":["def corrupt_image(input_pdf_path, output_pdf_path, border=50, noise=0.1, spot=(100,100), scale=0.95, theta=0.2, blur=0.5):\n","    \"\"\"Applies transformations to pdf in order to make the document harder to parse\"\"\"\n","\n","    image = load_image(input_pdf_path, zoom_x=1.5, zoom_y=1.5)\n","\n","    width, height = image.size\n","\n","    # # imitate curled page by making the top-right and bottom-left corners go slightly up and darkening the edges\n","    if border is not None:\n","        for x in range(1,border):\n","            tone = 256 - int(250*(x/border-1)**2)\n","            for y in range(height):\n","                # only update color if the pixel is white\n","                if min(image.getpixel((x,y))) < 20:\n","                    continue\n","                image.putpixel((x, y), (tone, tone, tone))\n","                image.putpixel((width-x, y), (tone, tone, tone))\n","\n","    # Apply noise\n","    if noise is not None:\n","        draw = ImageDraw.Draw(image)\n","        for _ in range(int(width * height * noise)):\n","            x = random.randint(0, width - 1)\n","            y = random.randint(0, height - 1)\n","            # TODO: Parameterize\n","            delta = random.gauss(0,10)\n","            rgb = tuple([int(min(max(0,val+delta),256)) for val in image.getpixel((x,y))])\n","            draw.point((x, y), fill=rgb)\n","\n","    if spot is not None:\n","        draw = ImageDraw.Draw(image)\n","        for _ in range(int(width * height * noise)):\n","            x = random.randint(0, width - 1)\n","            y = random.randint(0, height - 1)\n","            #TODO: Parameterize\n","            delta = 10000 / (1 + math.sqrt((spot[0]-x)**2 + (spot[1]-y)**2))\n","            rgb = tuple([int(min(max(0,val-delta),256)) for val in image.getpixel((x,y))])\n","            draw.point((x, y), fill=rgb)\n","\n","    # rescale the image within 10% to 20%\n","    if scale is not None:\n","        image = image.resize(size=(int(scale*width), int(scale*height)))\n","\n","    # apply a rotation\n","    if theta is not None:\n","        image = image.rotate(theta, expand=True)\n","\n","    # Apply blur\n","    if blur is not None:\n","        image = image.filter(ImageFilter.GaussianBlur(blur))\n","\n","    # Save processed images back as a PDF\n","    image.save(output_pdf_path, \"PDF\", resolution=100.0, save_all=True)\n","\n","    print(f\"Saved {output_pdf_path}\")\n","\n","noisy_pdf_filename = 'noisy_invoice.pdf'\n","corrupt_image( pdf_filename, noisy_pdf_filename)\n"]},{"cell_type":"markdown","metadata":{"id":"_rHjuG6JD--P"},"source":["## Quick sanity check\n","Here we overlay the ground truth labels with the noisy document"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1067,"status":"ok","timestamp":1702505781372,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"ONwxhTaXD8jH","outputId":"ca81f797-b1a6-4141-b0e0-296922d25bb7"},"outputs":[],"source":["def draw_boxes(image, response, color='red'):\n","    \"\"\"Draws boxes around text on the image\"\"\"\n","    draw = ImageDraw.Draw(image)\n","    for item in response:\n","        draw.rectangle(item['position'], outline=color)\n","    return image\n","\n","def scale_data(data: List[dict], w: int, h: int):\n","    \"\"\"\n","    Rescales the position data so that it matches the image size.\n","    In other words, convert position from relative to page size to absolute.\n","\n","    Args:\n","        data (List[dict]): List of dictionaries containing the position, font and text of each section\n","        w (int): Width of the image\n","        h (int): Height of the image\n","\n","    Returns:\n","        List[dict]: List of dictionaries containing the position, font and text of each section\n","    \"\"\"\n","    scaled_data = []\n","    for section in data:\n","        entry = section.copy()\n","        p = section['position']\n","        # convert to absolute units in pixels\n","        entry['position'] = [p[0]*w, p[3]*h, p[2]*w, p[1]*h]\n","        scaled_data.append(entry)\n","\n","    return scaled_data\n","\n","# Here we show our ground truth labels overlaid on the document\n","noisy_image = load_image(noisy_pdf_filename, zoom_x=1.5, zoom_y=1.5)\n","scaled_data = scale_data(data, *noisy_image.size)\n","draw_boxes(noisy_image.copy(), scaled_data, color='green')"]},{"cell_type":"markdown","metadata":{"id":"jcwFaIjwJnBj"},"source":["# 2. Incentive mechanism\n","\n","\n","- Simplest option is just to measure the similarity between true text and the OCR model's predicted text.\n","- More challenging is require miners to provide positional information about text (bounding boxes/ sections of text)\n","- More challenging still is to provide 'metadata' about text (font, size, color, etc.). Possibly even equations, tables, etc.\n","- Ideal outcome is to get the miner to give html + css to reproduce the exact document (denoised). Even some way of handling embedded images.\n","\n","**Use case for this would be to easily extract and reproduce a webpage, presentation slide or even a photo of an object.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9eZb65nqJnBj"},"source":["Loss function for OCR model:\n","\n","$$ L = \\sum_i \\alpha_p L^p_i + \\alpha_f L^f_i + \\alpha_t L^t_i $$\n","\n","where\n","\n","$ L^p_i $ is the loss for section i based on positional/layout correctness. This should be zero if the OCR model returns the exact box on the page.\n","\n","We propose that the positional loss is the intersection over union of the bounding boxes:\n","$$ L^p_i = IOU(\\hat{b}_i, b_i) $$\n","\n","where $ \\hat{b}_i $ is the predicted bounding box and $ b_i $ is the ground truth bounding box.\n","\n","\n","$ L^f_i $ is the loss for section i based on font correctness. This should be zero if the OCR model returns the exact font for the section, including font family, font size and perhaps even colors.\n","\n","We propose that the font loss is a delta between the predicted font and the ground truth font plus the square of the difference in font size:\n","$$ L^f_i = \\alpha_f^f (1 - \\delta(\\hat{f}_i, f_i) )+ \\alpha_f^s (\\hat{s}_i - s_i)^2 $$\n","\n","$ L^t_i $ is the loss for section i based on text correctness. This should be zero if the OCR model returns the exact text for the section.\n","\n","We propose that the text loss is the edit distance between the predicted text and the ground truth text:\n","$$ L^t_i = ED(\\hat{t}_i, t_i) $$\n","\n","where $ ED $ is the edit distance function. This is equivalent to the Levenshtein distance.\n","\n","$ \\alpha_p, \\alpha_f, \\alpha_t $ are weights for each of the loss terms. These will impact the difficulty of the OCR challenge as text correctness is likely much easier than position correctness etc.\n","\n","We will convert the loss to produce a reward which is to be maximized by the miner. To do this we will trivially subtract the loss from 1 for each term.\n","\n","$$ R = \\sum_i \\alpha_p (1 - L^p_i) + \\alpha_f (1 - L^f_i) + \\alpha_t (1 - L^t_i) $$\n","\n","where $ L $ is the loss function defined above. This probably some epsilon to avoid division by zero.\n","\n","Lastly, we will include a time penalty to encourage miners to respond quickly. This will be a linear penalty based on the time taken to respond.\n","\n","$$ R_{total} = \\alpha_{prediction}  R + \\alpha_{time} t $$\n","\n","where $ t $ is the time taken to respond."]},{"cell_type":"markdown","metadata":{"id":"woUtA5LxJnBj"},"source":["## A Note on desired schema\n","\n","As we want to score based on attributes beyond just the text we require that miners respond with the following schema:\n","```python\n","response = [\n","    {'index':0, 'position':[x1, x2, y1, y2], 'font_family':'Times New Roman', 'font_size':12, 'text':'Hello World!'},\n","    ...\n","]\n","```\n","\n","We can also build in some deisrable default behaviour in case the miner is unable to do the task in the desired way:\n","- If response is a `str`, then we just assume that the order of sections is correct and the text is correct.\n","- If response is a `List[str]`, then we assume that the order of sections is correct but the text is not.\n","- If response is a `List[dict]`, then we assume that the miner has provided all the information we need.\n","\n","Missing fields incur the maximum loss for that field.\n","\n","After some experimentation we find that the reward is highly sensitive to the order of the sections. This is not ideal as it means that the miner must also predict the order of the sections. We can solve this by automatically sort the predicted sections using the Hungarian algorithm. This is a simple linear assignment problem which will minimize the total distance between the predicted and ground truth sections."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1702505781373,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"M1pivmxKJnBk"},"outputs":[],"source":["\n","\n","\n","def get_position_reward(boxA: List[float], boxB: List[float] = None):\n","    \"\"\"\n","    Calculate the intersection over union (IoU) of two bounding boxes.\n","\n","    Args:\n","    - boxA (list): Bounding box coordinates of box A in the format [x1, y1, x2, y2].\n","    - boxB (list): Bounding box coordinates of box B in the format [x1, y1, x2, y2].\n","\n","    Returns:\n","    - float: The IoU value, ranging from 0 to 1.\n","    \"\"\"\n","    if not boxB:\n","        return 0.0\n","\n","    xA = max(boxA[0], boxB[0])\n","    yA = max(boxA[1], boxB[1])\n","    xB = min(boxA[2], boxB[2])\n","    yB = min(boxA[3], boxB[3])\n","\n","    intersection_area = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n","\n","    boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n","    boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n","\n","    iou = intersection_area / float(boxA_area + boxB_area - intersection_area)\n","\n","    return iou\n","\n","def get_text_reward(text1: str, text2: str = None):\n","    \"\"\"\n","    Calculate the edit distance between two strings.\n","\n","    Args:\n","    - text1 (str): The first string.\n","    - text2 (str): The second string.\n","\n","    Returns:\n","    - float: The edit distance between the two strings. Normalized to be between 0 and 1.\n","    \"\"\"\n","    if not text2:\n","        return 0.0\n","\n","    return 1 - editdistance.eval(text1, text2) / max(len(text1), len(text2))\n","\n","def get_font_reward(font1: dict, font2: dict = None, alpha_size=1.0, alpha_family=1.0):\n","    \"\"\"\n","    Calculate the distance between two fonts, based on the font size and font family.\n","\n","    Args:\n","    - font1 (dict): The first font.\n","    - font2 (dict): The second font.\n","\n","    Returns:\n","    - float: The distance between the two fonts. Normalized to be between 0 and 1.\n","    \"\"\"\n","    if not font2:\n","        return 0.0\n","\n","    font_size_score = ( 1 - abs(font1['size'] - font2['size']) / max(font1['size'], font2['size']) )\n","    font_family_score = alpha_family * float(font1['family'] == font2['family'])\n","    return (alpha_size * font_size_score + alpha_family * font_family_score) / (alpha_size + alpha_family)\n","\n","def section_reward(label: dict, pred: dict, alpha_p=1.0, alpha_f=1.0, alpha_t=1.0, verbose=False):\n","    \"\"\"\n","    Score a section of the image based on the section's correctness.\n","    Correctness is defined as:\n","    - the intersection over union of the bounding boxes,\n","    - the delta between the predicted font and the ground truth font,\n","    - and the edit distance between the predicted text and the ground truth text.\n","\n","    Args:\n","    - label (dict): The ground truth data for the section.\n","    - pred (dict): The predicted data for the section.\n","\n","    Returns:\n","    - float: The score for the section. Bounded between 0 and 1.\n","    \"\"\"\n","    reward = {\n","        'text': get_text_reward(label['text'], pred.get('text')),\n","        'position': get_position_reward(label['position'], pred.get('position')),\n","        'font': get_font_reward(label['font'], pred.get('font')),\n","    }\n","\n","    reward['total'] = (alpha_t * reward['text'] + alpha_p * reward['position'] + alpha_f * reward['font']) / (alpha_p + alpha_f + alpha_t)\n","\n","    if verbose:\n","        bt.logging.info(', '.join([f\"{k}: {v:.3f}\" for k,v in reward.items()]))\n","\n","    return reward\n","\n","def sort_predictions(image_data: List[dict], predictions: List[dict], draw=False) -> List[dict]:\n","    \"\"\"\n","    Sort the predictions to match the order of the ground truth data using the Hungarian algorithm.\n","\n","    Args:\n","    - image_data (list): The ground truth data for the image.\n","    - predictions (list): The predicted data for the image.\n","\n","    Returns:\n","    - list: The sorted predictions.\n","    \"\"\"\n","\n","    # First, make sure that the predictions is at least as long as the image data\n","    predictions += [{}] * (len(image_data) - len(predictions))\n","    r = torch.zeros((len(image_data), len(predictions)))\n","    for i in range(r.shape[0]):\n","        for j in range(r.shape[1]):\n","            r[i,j] = section_reward(image_data[i], predictions[j])['total']\n","\n","    # Use the Hungarian algorithm to find the best assignment\n","    row_indices, col_indices = linear_sum_assignment(r, maximize=True)\n","\n","    if draw:\n","        fig = px.imshow(r.detach().numpy(),\n","                    color_continuous_scale='Blues',\n","                    title=f'Optimal Assignment (Avg. Reward: {r[row_indices, col_indices].mean():.3f})',\n","                    width=600, height=600\n","                    )\n","        fig.update_layout(coloraxis_showscale=False)\n","        fig.update_yaxes(title_text='Ground Truth')\n","        fig.update_xaxes(title_text='Predictions')\n","\n","        for i, j in zip(row_indices, col_indices):\n","            fig.add_annotation(x=j, y=i, text='+', showarrow=False, font=dict(color='red', size=16))\n","        fig.show()\n","\n","    sorted_predictions = [predictions[i] for i in col_indices]\n","\n","    return sorted_predictions\n","\n","\n","def reward(image_data: List[dict], predictions: List[dict], time_elapsed: float) -> float:\n","    \"\"\"\n","    Reward the miner response to the OCR request.\n","\n","    Args:\n","    - image_data (list): The ground truth data for the image.\n","    - predictions (list): The predicted data for the image.\n","    - time_elapsed (float): Time taken for miner to extract data from image\n","\n","    Returns:\n","    - float: The reward for the miner response. Bounded between 0 and 1.\n","    \"\"\"\n","\n","    if predictions is None:\n","        return 0.0\n","\n","    # Sort the predictions to match the order of the ground truth data as best as possible\n","    predictions = sort_predictions(image_data, predictions)\n","\n","    # Take mean score over all sections in document (note that we don't penalize extra sections)\n","    section_rewards = [section_reward(label, pred, verbose=True) for label, pred in zip(image_data, predictions)]\n","    prediction_reward = torch.mean(torch.FloatTensor([r['total'] for r in section_rewards]))\n","\n","    alpha_prediction = 0.8\n","    alpha_time = 0.2\n","    max_time = 5\n","    time_reward = max(1 - time_elapsed / max_time, 0)\n","    total_reward = (alpha_prediction * prediction_reward + alpha_time * time_reward) / (alpha_prediction + alpha_time)\n","\n","    bt.logging.info(f\"prediction_reward: {prediction_reward:.3f}, time_reward: {time_reward:.3f}, total_reward: {total_reward:.3f}\")\n","    return total_reward"]},{"cell_type":"markdown","metadata":{"id":"hhKFy5U2EW7e"},"source":["# 3. Miner\n","For the miner we will define a baseline approach which uses pytesseract. Of course, if this were a real subnet we would expect and encourage miners to utilize more sophisticated appraoches to OCR (such as deep learning models)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1212,"status":"ok","timestamp":1702505782578,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"vIU0tua771eg","outputId":"9f5443bf-0f59-4bdd-cdb4-b4f8f16e851c"},"outputs":[],"source":["!apt install tesseract-ocr"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8154,"status":"ok","timestamp":1702505790731,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"JKT-Ekuj79AA","outputId":"dcb3a9c6-70dc-4758-8d27-187b62840f24"},"outputs":[],"source":["!pip install pytesseract"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1737,"status":"ok","timestamp":1702505792464,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"qImV1E-Z8e8i","outputId":"797a29f5-0f4c-4905-ad31-bb25d851c2ce"},"outputs":[],"source":["import pytesseract\n","\n","\n","def group_and_merge_boxes(data, xtol=25, ytol=5):\n","    \"\"\"\n","    Combines boxes that are close together into a single box so that the text is grouped into sections.\n","\n","    Args:\n","    - data (list): List of dictionaries containing the position, font and text of each section\n","    - xtol (int): Maximum distance between boxes in the x direction to be considered part of the same section\n","    - ytol (int): Maximum distance between boxes in the y direction to be considered part of the same section\n","\n","    Returns:\n","    - list: List of dictionaries containing the position, font and text of each section\n","    \"\"\"\n","    # Ensure all data items are valid and have a 'position' key\n","    data = [box for box in data if box is not None and 'position' in box]\n","\n","    # Step 1: Group boxes by lines\n","    lines = []\n","    for box in data:\n","        added_to_line = False\n","        for line in lines:\n","            if line and abs(line[0]['position'][1] - box['position'][1]) <= ytol:\n","                line.append(box)\n","                added_to_line = True\n","                break\n","        if not added_to_line:\n","            lines.append([box])\n","\n","    # Step 2: Sort and merge within each line\n","    merged_data = []\n","    for line in lines:\n","        line.sort(key=lambda item: item['position'][0])  # Sort by x1\n","        i = 0\n","        while i < len(line) - 1:\n","            box1 = line[i]['position']\n","            box2 = line[i + 1]['position']\n","            if abs(box1[2] - box2[0]) <= xtol:  # Check horizontal proximity\n","                new_box = {'position': [min(box1[0], box2[0]), min(box1[1], box2[1]), max(box1[2], box2[2]), max(box1[3], box2[3])],\n","                           'text': line[i]['text'] + ' ' + line[i + 1]['text']}\n","                line[i] = new_box\n","                del line[i + 1]\n","            else:\n","                i += 1\n","        merged_data.extend(line)\n","\n","    return merged_data\n","\n","\n","def miner(image, merge=True, sort=True):\n","    \"\"\"\n","    Extracts text data from image using pytesseract. This is the baseline miner.\n","    \"\"\"\n","\n","    # Use pytesseract to get the data\n","    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n","\n","    response = []\n","\n","    for i in range(len(data['text'])):\n","        if data['text'][i].strip() != '':  # This filters out empty text results\n","            x1, y1, width, height = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n","            if width * height < 10:  # This filters out small boxes (likely noise)\n","                continue\n","\n","            x2, y2 = x1 + width, y1 + height\n","\n","            # Here we don't have font information, so we'll omit that.\n","            # Pytesseract does not extract font family or size information.\n","            entry = {\n","                'position': [x1, y1, x2, y2],\n","                'text': data['text'][i]\n","            }\n","            response.append(entry)\n","\n","    # Merge together words into sections, which are on the same line (same y value) and are close together (small distance in x)\n","    if merge:\n","        response = group_and_merge_boxes(response)\n","\n","    # Sort sections by y, then sort by x so that they read left to right and top to bottom\n","    if sort:\n","        response = sorted(response, key=lambda item: (item['position'][1], item['position'][0]))\n","\n","    # Now 'response' will be a list of dictionaries with the sections of text and the accompanying bounding box coordinates.\n","    return response\n","\n","\n","tbeg = time.time()\n","response = miner(noisy_image, merge=True)\n","time_elapsed = time.time() - tbeg\n","print(f'Miner parsed document in {time_elapsed:.2f} seconds')\n","response[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":908,"status":"ok","timestamp":1702505793367,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"FhmcdFfKFuzc","outputId":"5f16814f-6d12-49ac-d608-e6f413b89ee0"},"outputs":[],"source":["image_copy = noisy_image.copy()\n","draw_boxes(image_copy, scaled_data, color='green')\n","draw_boxes(image_copy, response, color='red')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1702505793561,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"A39hbnsCztT6","outputId":"a7fc02bb-c58a-4f3c-cd80-ab2f321f9ebc"},"outputs":[],"source":["# Show how predictions are sorted to match the ground truth\n","sort_predictions(scaled_data, response, draw=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1702505793561,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"ROh4hL8wE80b","outputId":"ba332fdd-5231-4547-c49e-9da33b16f6e1"},"outputs":[],"source":["# Test the reward function with slightly modified image data\n","reward(scaled_data, response, time_elapsed)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702505793561,"user":{"displayName":"Raj K","userId":"04232075365376767238"},"user_tz":480},"id":"pBNKY-4qztT6","outputId":"b76aec2b-50c9-45c0-adf1-667924a497aa"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
